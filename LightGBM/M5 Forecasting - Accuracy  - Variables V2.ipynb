{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "M5 Forecasting - Accuracy\n",
    "\n",
    "Note: This is one of the two complementary competitions that together comprise the M5 forecasting challenge. Can you estimate, as precisely as possible, the point forecasts of the unit sales of various products sold in the USA by Walmart?\n",
    "\n",
    "How much camping gear will one store sell each month in a year? To the uninitiated, calculating sales at this level may seem as difficult as predicting the weather. Both types of forecasting rely on science and historical data. While a wrong weather forecast may result in you carrying around an umbrella on a sunny day, inaccurate business forecasts could result in actual or opportunity losses. In this competition, in addition to traditional forecasting methods you’re also challenged to use machine learning to improve forecast accuracy.\n",
    "\n",
    "The Makridakis Open Forecasting Center (MOFC) at the University of Nicosia conducts cutting-edge forecasting research and provides business forecast training. It helps companies achieve accurate predictions, estimate the levels of uncertainty, avoiding costly mistakes, and apply best forecasting practices. The MOFC is well known for its Makridakis Competitions, the first of which ran in the 1980s.\n",
    "\n",
    "In this competition, the fifth iteration, you will use hierarchical sales data from Walmart, the world’s largest company by revenue, to forecast daily sales for the next 28 days. The data, covers stores in three US States (California, Texas, and Wisconsin) and includes item level, department, product categories, and store details. In addition, it has explanatory variables such as price, promotions, day of the week, and special events. Together, this robust dataset can be used to improve forecasting accuracy.\n",
    "\n",
    "If successful, your work will continue to advance the theory and practice of forecasting. The methods used can be applied in various business areas, such as setting up appropriate inventory or service levels. Through its business support and training, the MOFC will help distribute the tools and knowledge so others can achieve more accurate and better calibrated forecasts, reduce waste and be able to appreciate uncertainty and its risk implications.\n",
    "\n",
    "Evaluation:\n",
    "This competition uses a Weighted Root Mean Squared Scaled Error (RMSSE). Extensive details about the metric, scaling, and weighting can be found in the [M5 Participants Guide](https://mofc.unic.ac.cy/m5-competition/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to notice that this notebook must be seen together with the notebook \"M5 Forecasting - Accuracy Data study.ipynb\".\\\n",
    "This notebook have only the constructions of new variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is the same as \"M5 Forecasting - Accuracy - Data study\".\\\n",
    "The only difference is that the variables are construct thinking in the Light GBM model.\\\n",
    "Therefore, e.g., we will use categorical variables, but we will not transform the bins in dummies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:\\\\Users\\\\maxwi\\\\Python\\\\Kaggle\\\\M5 Forecasting - Accuracy\\\\Modelo 2\\\\data_by_id\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function to reduce memory usage.\n",
    "From: https://www.kaggle.com/ragnar123/very-fst-model\n",
    "'''\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to  0.12 Mb (41.9% reduction)\n",
      "Mem. usage decreased to 130.48 Mb (37.5% reduction)\n",
      "Mem. usage decreased to 95.00 Mb (78.7% reduction)\n"
     ]
    }
   ],
   "source": [
    "#Load data.\n",
    "calendar = pd.read_csv('calendar.csv')\n",
    "calendar = reduce_mem_usage(calendar)\n",
    "sell_prices = pd.read_csv('sell_prices.csv')\n",
    "sell_prices = reduce_mem_usage(sell_prices)\n",
    "sales_train_validation = pd.read_csv('sales_train_validation.csv')\n",
    "sales_train_validation = reduce_mem_usage(sales_train_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File 1: “calendar.csv”\n",
    "\n",
    "Contains information about the dates the products are sold.\n",
    "\n",
    "     date: The date in a “y-m-d” format.\n",
    "\n",
    "     wm_yr_wk: The id of the week the date belongs to.\n",
    "    \n",
    "     weekday: The type of the day (Saturday, Sunday, …, Friday).\n",
    "    \n",
    "     wday: The id of the weekday, starting from Saturday.\n",
    "    \n",
    "     month: The month of the date.\n",
    "    \n",
    "     year: The year of the date.\n",
    "    \n",
    "     event_name_1: If the date includes an event, the name of this event.\n",
    "    \n",
    "     event_type_1: If the date includes an event, the type of this event.\n",
    "    \n",
    "     event_name_2: If the date includes a second event, the name of this event.\n",
    "    \n",
    "     event_type_2: If the date includes a second event, the type of this event.\n",
    "    \n",
    "     snap_CA, snap_TX, and snap_WI: A binary variable (0 or 1) indicating whether the stores of CA, TX or WI allow SNAP2 purchases on the examined date. 1 indicates that SNAP purchases are allowed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File 2: “sell_prices.csv”\n",
    "\n",
    "Contains information about the price of the products sold per store and date.\n",
    "\n",
    "     store_id: The id of the store where the product is sold.\n",
    "\n",
    "     item_id: The id of the product.\n",
    "\n",
    "     wm_yr_wk: The id of the week.\n",
    "\n",
    "     sell_price: The price of the product for the given week/store. The price is provided per week (average across seven days). If not available, this means that the product was not sold during the examined week. Note that although prices are constant at weekly basis, they may change through time (both training and test set).\n",
    "    \n",
    "    \n",
    "Considering that we have only 6,841,121 prices, there are  1,757,059 (8,598,180 - 6,841,121) products that were not sold in a given week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File 3: “sales_train.csv”\n",
    "\n",
    "Contains the historical daily unit sales data per product and store.\n",
    "\n",
    "     item_id: The id of the product.\n",
    "\n",
    "     dept_id: The id of the department the product belongs to.\n",
    "\n",
    "     cat_id: The id of the category the product belongs to.\n",
    "\n",
    "     store_id: The id of the store where the product is sold.\n",
    "\n",
    "     state_id: The State where the store is located.\n",
    "\n",
    "     d_1, d_2, …, d_i, … d_1941: The number of units sold at day i, starting from 2011-01-29.\n",
    "    \n",
    "    \n",
    "We have 282 and 30,490 differents items. Therefore, the ideal would be to have 8,598,180 (282 x 30,400) prices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to  0.29 Mb (4.3% reduction)\n"
     ]
    }
   ],
   "source": [
    "#Variables based on calendar\n",
    "#Seasons\n",
    "calendar['date'] = pd.to_datetime(calendar['date'])\n",
    "calendar['day'] = calendar['date'].dt.day\n",
    "\n",
    "calendar.loc[np.in1d(calendar['month'], [3, 4, 5]), 'season'] = 1 #'spring'\n",
    "calendar.loc[np.in1d(calendar['month'], [6, 7, 8]), 'season'] = 2 #'summer'\n",
    "calendar.loc[np.in1d(calendar['month'], [9, 10, 11]), 'season'] = 3 #'fall'\n",
    "calendar.loc[np.in1d(calendar['month'], [12, 1, 2]), 'season'] = 4 #'winter'\n",
    "calendar['season'] = calendar['season'].astype('int8')\n",
    "\n",
    "calendar.loc[np.in1d(calendar['day'], [1, 2, 3, 4, 5, 6, 7]), 'month_fase'] = 1# 'start'\n",
    "calendar.loc[np.in1d(calendar['day'], [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]), 'month_fase'] = 2 #'middle'\n",
    "calendar.loc[np.in1d(calendar['day'], [22, 23, 24, 25, 26, 27, 28, 29, 30, 31]), 'month_fase'] = 3 #'end'\n",
    "calendar['month_fase'] = calendar['month_fase'].astype('int8')\n",
    "\n",
    "calendar.loc[np.in1d(calendar['wday'], [1, 2]), 'week_fase'] = 1 #'weekend'\n",
    "calendar.loc[np.in1d(calendar['wday'], [3, 4, 5]), 'week_fase'] = 2 #'start'\n",
    "calendar.loc[np.in1d(calendar['wday'], [6, 7]), 'week_fase'] = 3 #'end'\n",
    "calendar['week_fase'] = calendar['week_fase'].astype('int8')\n",
    "\n",
    "calendar.drop(['date', 'weekday', 'year'], inplace = True, axis = 1)\n",
    "\n",
    "#Fix variables event_name and event_type\n",
    "#Make new holiday variable\n",
    "calendar['event_name_1_t3'] = calendar['event_name_1'].shift(-3)\n",
    "calendar['event_name_2_t3'] = calendar['event_name_2'].shift(-3)\n",
    "calendar['event_name_1_t7'] = calendar['event_name_1'].shift(-7)\n",
    "calendar['event_name_2_t7'] = calendar['event_name_2'].shift(-7)\n",
    "\n",
    "calendar['event_type_1_t3'] = calendar['event_type_1'].shift(-3)\n",
    "calendar['event_type_2_t3'] = calendar['event_type_2'].shift(-3)\n",
    "calendar['event_type_1_t7'] = calendar['event_type_1'].shift(-7)\n",
    "calendar['event_type_2_t7'] = calendar['event_type_2'].shift(-7)\n",
    "\n",
    "#Categorize holidays\n",
    "def cat_holidays(holiday):\n",
    "    calendar[holiday] = np.logical_or(np.in1d(calendar['event_name_1'], holiday), np.in1d(calendar['event_name_2'], holiday))\n",
    "    \n",
    "    calendar[holiday + '_t3'] = np.logical_or(np.in1d(calendar['event_name_1_t3'], holiday)\n",
    "                                                               , np.in1d(calendar['event_name_2_t3'], holiday))\n",
    "    \n",
    "    calendar[holiday + '_t7'] = np.logical_or(np.in1d(calendar['event_name_1_t7'], holiday)\n",
    "                                                               , np.in1d(calendar['event_name_2_t7'], holiday))\n",
    "    \n",
    "    calendar[holiday + '_near'] = 0\n",
    "    calendar[holiday + '_near'] = calendar[holiday + '_t3'].rolling(4).sum()\n",
    "    calendar[holiday + '_near'].fillna(0, inplace = True)\n",
    "    \n",
    "    calendar[holiday + '_week'] = 0\n",
    "    calendar[holiday + '_week'] = calendar[holiday + '_t7'].rolling(8).sum()\n",
    "    calendar[holiday + '_week'].fillna(0, inplace = True)\n",
    "    calendar[holiday + '_weekend']  = np.logical_and(calendar[holiday + '_week'], np.in1d(calendar['week_fase'], 1))\n",
    "    \n",
    "    calendar[holiday + '_near'] = calendar[holiday + '_near'].astype('bool')\n",
    "    calendar[holiday + '_week'] = calendar[holiday + '_week'].astype('bool')\n",
    "    \n",
    "    calendar.drop([holiday + '_t3', holiday + '_t7'], inplace = True, axis = 1)\n",
    "        \n",
    "        \n",
    "def cat_holiday_type(holiday_type):\n",
    "    calendar[holiday_type] = np.logical_or(np.in1d(calendar['event_type_1'], holiday_type), np.in1d(calendar['event_type_2'], holiday_type))\n",
    "      \n",
    "    calendar[holiday_type + '_t3'] = np.logical_or(np.in1d(calendar['event_type_1_t3'], holiday_type)\n",
    "                                                               , np.in1d(calendar['event_type_2_t3'], holiday_type))\n",
    "    \n",
    "    calendar[holiday_type + '_t7'] = np.logical_or(np.in1d(calendar['event_type_1_t7'], holiday_type)\n",
    "                                                               , np.in1d(calendar['event_type_2_t7'], holiday_type))\n",
    "    \n",
    "    calendar[holiday_type + '_near'] = 0\n",
    "    calendar[holiday_type + '_near'] = calendar[holiday_type + '_t3'].rolling(4).sum()\n",
    "    calendar[holiday_type + '_near'].fillna(0, inplace = True)\n",
    "    \n",
    "    calendar[holiday_type + '_week'] = 0\n",
    "    calendar[holiday_type + '_week'] = calendar[holiday_type + '_t7'].rolling(8).sum()\n",
    "    calendar[holiday_type + '_week'].fillna(0, inplace = True)\n",
    "    calendar[holiday_type + '_weekend']  = np.logical_and(calendar[holiday_type + '_week'], np.in1d(calendar['week_fase'], 1))\n",
    "      \n",
    "    calendar[holiday_type + '_near'] = calendar[holiday_type + '_near'].astype('bool')\n",
    "    calendar[holiday_type + '_week'] = calendar[holiday_type + '_week'].astype('bool')\n",
    "        \n",
    "    calendar.drop([holiday_type + '_t3', holiday_type + '_t7'], inplace = True, axis = 1)\n",
    "\n",
    "    \n",
    "holidays = calendar['event_name_1'].unique()[1:]\n",
    "holiday_type = calendar['event_type_1'].unique()[1:]\n",
    "\n",
    "for e in holidays:\n",
    "    cat_holidays(e)\n",
    "    \n",
    "for e in holiday_type:\n",
    "    cat_holiday_type(e)\n",
    "    \n",
    "    \n",
    "calendar.drop(['event_name_1', 'event_name_2', 'event_type_1', 'event_type_2'\n",
    "               , 'event_name_1_t3', 'event_name_1_t7'\n",
    "               , 'event_name_2_t3', 'event_name_2_t7'\n",
    "               , 'event_type_1_t3', 'event_type_1_t7'\n",
    "               , 'event_type_2_t3', 'event_type_2_t7'], inplace = True, axis = 1)\n",
    "\n",
    "calendar = reduce_mem_usage(calendar)  #Mem. usage decreased to  0.29 Mb (4.3% reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transpose sales_train_validation so the days goes to rows.\n",
    "sales_train_validation = pd.melt(sales_train_validation, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name = 'day', value_name = 'demand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to  2.09 Mb (84.5% reduction)\n",
      "Mem. usage decreased to  2.09 Mb (84.5% reduction)\n"
     ]
    }
   ],
   "source": [
    "#Define 2 holdout\n",
    "holdout1 = pd.read_csv('sample_submission.csv')\n",
    "holdout1 = reduce_mem_usage(holdout1)\n",
    "holdout2 = pd.read_csv('sample_submission.csv')\n",
    "holdout2 = reduce_mem_usage(holdout2)\n",
    "\n",
    "\n",
    "#One holdout is related to forecast between day 1914 and 1941.\n",
    "#The second, between day 1942 and 1969\n",
    "test1 = holdout1.copy()\n",
    "test2 = holdout2.copy()\n",
    "test1.columns = ['id', 'd_1914', 'd_1915', 'd_1916', 'd_1917', 'd_1918', 'd_1919', 'd_1920', 'd_1921', 'd_1922', 'd_1923',\n",
    "                    'd_1924', 'd_1925', 'd_1926', 'd_1927', 'd_1928', 'd_1929', 'd_1930', 'd_1931', \n",
    "                    'd_1932', 'd_1933', 'd_1934', 'd_1935', 'd_1936', 'd_1937', 'd_1938', 'd_1939', 'd_1940', 'd_1941']\n",
    "test2.columns = ['id', 'd_1942', 'd_1943', 'd_1944', 'd_1945', 'd_1946', 'd_1947', 'd_1948', 'd_1949', 'd_1950', 'd_1951', \n",
    "                    'd_1952', 'd_1953', 'd_1954', 'd_1955', 'd_1956', 'd_1957', 'd_1958', 'd_1959', \n",
    "                    'd_1960', 'd_1961', 'd_1962', 'd_1963', 'd_1964', 'd_1965', 'd_1966', 'd_1967', 'd_1968', 'd_1969']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concat our train database with the holdouts. \n",
    "\n",
    "#Get information about the product id.\n",
    "product_id = sales_train_validation[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']].drop_duplicates()\n",
    "\n",
    "# merge with product table\n",
    "validation_bool = test1['id'].str.contains(\"_validation\") \n",
    "test1 = test1[validation_bool]\n",
    "test1 = test1.merge(product_id, how = 'left', on = 'id')\n",
    "test2 = test2[validation_bool]\n",
    "test2 = test2.merge(product_id, how = 'left', on = 'id')\n",
    "\n",
    "#Transpose so the days goes to rows.\n",
    "test1 = pd.melt(test1, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name = 'day', value_name = 'demand')\n",
    "test2 = pd.melt(test2, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name = 'day', value_name = 'demand')\n",
    "\n",
    "#Define what is train and what is test.\n",
    "sales_train_validation['part'] = 'train'\n",
    "test1['part'] = 'test1'\n",
    "test2['part'] = 'test2'   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concat sales_train_validation and test1. test2 will be concated after we develop some demand variables\n",
    "data = pd.concat([sales_train_validation, test1], axis = 0) \n",
    "del sales_train_validation, test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 8691.68 Mb (60.9% reduction)\n"
     ]
    }
   ],
   "source": [
    "#make new demand related variables\n",
    "#It is necessary a shift of 28 days so these variables can be calculated for the forecast.\n",
    "\n",
    "#The Light GBM does not consider that our database is in a time series format. \n",
    "#Therefore, after we model one day, we will need to recalculate some variables to model the next day.\n",
    "\n",
    "for i in range(1, 29):\n",
    "    try:\n",
    "        data['demand_lag_t' + str(i)] = data.groupby(['id'])['demand'].shift(i)\n",
    "    except:\n",
    "        data['demand_lag_t' + str(i)] = 0\n",
    "\n",
    "data['demand_rolling_mean_t28_7'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(7).mean())\n",
    "data['demand_rolling_mean_t28_30'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).mean())\n",
    "data['demand_rolling_mean_t28_90'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(90).mean())\n",
    "data['demand_rolling_mean_t28_180'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(180).mean())\n",
    "data['demand_rolling_mean_t28_365'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(365).mean())\n",
    "data['demand_rolling_std_t28_7'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(7).std())\n",
    "data['demand_rolling_std_t28_30'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).std())\n",
    "data['demand_rolling_std_t28_90'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(90).std())\n",
    "data['demand_rolling_std_t28_180'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(180).std())\n",
    "data['demand_rolling_std_t28_365'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(365).std())\n",
    "data['demand_rolling_skew_t28_30'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).skew())\n",
    "data['demand_rolling_kurt_t28_30'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).kurt())\n",
    "\n",
    "data = reduce_mem_usage(data) #Mem. usage decreased to 8691.68 Mb (60.9% reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concat data and test2\n",
    "data = pd.concat([data, test2], axis = 0, sort = True)\n",
    "del test2\n",
    "\n",
    "#Merge with calendar.\n",
    "data = pd.merge(data, calendar, how = 'left', left_on = ['day'], right_on = ['d'])\n",
    "del calendar\n",
    "gc.collect()\n",
    "data.drop(['d'], inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Merge with sell_prices.\n",
    "data = pd.merge(data, sell_prices, how = 'left', on = ['store_id', 'item_id', 'wm_yr_wk'])\n",
    "\n",
    "#Delete rows with products that had not have yet your first sell.\n",
    "data = data.dropna(subset=['sell_price']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make new price related variables\n",
    "def create_bins_price_variation(df, var_name_in):\n",
    "    cut_points = [-999, -0.25, -0.10, -0.05, -0.001, 0.05, 0.10, 0.25, 999]\n",
    "    label_names = [\"--0.25\", \"-0.25-0.10\", \"-0.10-0.5\", \"-0.05-0\", \"0-0.05\", \"0.05-0.10\", \"0.10-0.25\", \"0.25++\"]\n",
    "    df[var_name_in + '_bin'] = pd.cut(df[var_name_in], cut_points, labels = label_names)\n",
    "    return df\n",
    "\n",
    "\n",
    "data['lag_price_t1'] = data.groupby(['id'])['sell_price'].transform(lambda x: x.shift(1))\n",
    "data['price_change_t1'] = (data['sell_price'] - data['lag_price_t1']) / (data['lag_price_t1'])\n",
    "data.drop(['lag_price_t1'], inplace = True, axis = 1)\n",
    "\n",
    "#Create bins for price change\n",
    "data = create_bins_price_variation(data, 'price_change_t1')\n",
    "\n",
    "\n",
    "#t4 = four weeks, one month;\n",
    "#t13 = 13 weeks, three months;\n",
    "#t26 = 26 weeks, six month;\n",
    "#t52 = 52 weeks, one year.\n",
    "data['rolling_price_max_t4'] = data.groupby(['id'])['sell_price'].transform(lambda x: x.shift(1).rolling(4).max())\n",
    "data['rolling_price_max_t13'] = data.groupby(['id'])['sell_price'].transform(lambda x: x.shift(1).rolling(13).max())\n",
    "data['rolling_price_max_t26'] = data.groupby(['id'])['sell_price'].transform(lambda x: x.shift(1).rolling(26).max())\n",
    "data['rolling_price_max_t52'] = data.groupby(['id'])['sell_price'].transform(lambda x: x.shift(1).rolling(52).max())\n",
    "\n",
    "data['price_change_max_t4'] = (data['sell_price'] - data['rolling_price_max_t4']) / (data['rolling_price_max_t4'])\n",
    "data['price_change_max_t13'] = (data['sell_price'] - data['rolling_price_max_t13']) / (data['rolling_price_max_t13'])\n",
    "data['price_change_max_t26'] = (data['sell_price'] - data['rolling_price_max_t26']) / (data['rolling_price_max_t26'])\n",
    "data['price_change_max_t52'] = (data['sell_price'] - data['rolling_price_max_t52']) / (data['rolling_price_max_t52'])\n",
    "\n",
    "data.drop(['rolling_price_max_t4', 'rolling_price_max_t13', 'rolling_price_max_t26', 'rolling_price_max_t52'], inplace = True, axis = 1)\n",
    "\n",
    "#Create bins for max price change\n",
    "data = create_bins_price_variation(data, 'price_change_max_t4')\n",
    "data = create_bins_price_variation(data, 'price_change_max_t13')\n",
    "data = create_bins_price_variation(data, 'price_change_max_t26')\n",
    "data = create_bins_price_variation(data, 'price_change_max_t52')\n",
    "\n",
    "\n",
    "data['rolling_price_std_t4'] = data.groupby(['id'])['sell_price'].transform(lambda x: x.rolling(4).std())\n",
    "data['rolling_price_std_t13'] = data.groupby(['id'])['sell_price'].transform(lambda x: x.rolling(13).std())\n",
    "data['rolling_price_std_t26'] = data.groupby(['id'])['sell_price'].transform(lambda x: x.rolling(26).std())\n",
    "data['rolling_price_std_t52'] = data.groupby(['id'])['sell_price'].transform(lambda x: x.rolling(52).std())\n",
    "\n",
    "data['rolling_price_mean_t4'] = data.groupby(['id'])['sell_price'].transform(lambda x: x.rolling(4).mean())\n",
    "data['rolling_price_mean_t13'] = data.groupby(['id'])['sell_price'].transform(lambda x: x.rolling(13).mean())\n",
    "data['rolling_price_mean_t26'] = data.groupby(['id'])['sell_price'].transform(lambda x: x.rolling(26).mean())\n",
    "data['rolling_price_mean_t52'] = data.groupby(['id'])['sell_price'].transform(lambda x: x.rolling(52).mean())\n",
    "                                                                                                  \n",
    "data['price_change_mean_t4'] = (data['sell_price'] - data['rolling_price_mean_t4']) / (data['rolling_price_mean_t4'])\n",
    "data['price_change_mean_t13'] = (data['sell_price'] - data['rolling_price_mean_t13']) / (data['rolling_price_mean_t13'])\n",
    "data['price_change_mean_t26'] = (data['sell_price'] - data['rolling_price_mean_t26']) / (data['rolling_price_mean_t26'])                                                                                                   \n",
    "data['price_change_mean_t52'] = (data['sell_price'] - data['rolling_price_mean_t52']) / (data['rolling_price_mean_t52'])\n",
    "\n",
    "data.drop(['rolling_price_mean_t4', 'rolling_price_mean_t13', 'rolling_price_mean_t26', 'rolling_price_mean_t52'], inplace = True, axis = 1)\n",
    "\n",
    "\n",
    "gc.collect()\n",
    "#Create bins for mean price change                                                                                                  \n",
    "data = create_bins_price_variation(data, 'price_change_mean_t4')\n",
    "data = create_bins_price_variation(data, 'price_change_mean_t13')\n",
    "data = create_bins_price_variation(data, 'price_change_mean_t26')\n",
    "data = create_bins_price_variation(data, 'price_change_mean_t52')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 15660.27 Mb (10.9% reduction)\n",
      "Mem. usage decreased to 17213.41 Mb (4.5% reduction)\n",
      "Mem. usage decreased to 14982.73 Mb (0.0% reduction)\n"
     ]
    }
   ],
   "source": [
    "#Verify if there is a snap sales in other state.\n",
    "gc.collect()\n",
    "\n",
    "data['snap_other_CA'] = 0\n",
    "data['snap_other_CA_only'] = 0\n",
    "data.loc[(np.in1d(data['snap_TX'], 1)) | (np.in1d(data['snap_WI'], 1)), 'snap_other_CA'] = 1\n",
    "data.loc[(np.in1d(data['snap_other_CA'], 1)) & (np.in1d(data['snap_CA'], 0)), 'snap_other_CA_only'] = 1\n",
    "\n",
    "data['snap_other_TX'] = 0\n",
    "data['snap_other_TX_only'] = 0\n",
    "data.loc[(np.in1d(data['snap_CA'], 1)) | (np.in1d(data['snap_WI'], 1)), 'snap_other_TX'] = 1\n",
    "data.loc[(np.in1d(data['snap_other_TX'], 1)) & (np.in1d(data['snap_TX'], 0)), 'snap_other_TX_only'] = 1\n",
    "\n",
    "data['snap_other_WI'] = 0\n",
    "data['snap_other_WI_only'] = 0\n",
    "data.loc[(np.in1d(data['snap_CA'], 1)) | (np.in1d(data['snap_TX'], 1)), 'snap_other_WI'] = 1\n",
    "data.loc[(np.in1d(data['snap_other_WI'], 1)) & (np.in1d(data['snap_WI'], 0)), 'snap_other_WI_only'] = 1\n",
    "\n",
    "data = reduce_mem_usage(data) #Mem. usage decreased to 15660.27 Mb (10.9% reduction)\n",
    "\n",
    "#Set the right snap per product.\n",
    "data.loc[np.in1d(data['state_id'], 'CA'), 'snap'] = data['snap_CA']\n",
    "data.loc[np.in1d(data['state_id'], 'CA'), 'snap_other'] = data['snap_other_CA']\n",
    "data.loc[np.in1d(data['state_id'], 'CA'), 'snap_only_other'] = data['snap_other_CA_only']\n",
    "\n",
    "data.loc[np.in1d(data['state_id'], 'TX'), 'snap'] = data['snap_TX']\n",
    "data.loc[np.in1d(data['state_id'], 'TX'), 'snap_other'] = data['snap_other_TX']\n",
    "data.loc[np.in1d(data['state_id'], 'TX'), 'snap_only_other'] = data['snap_other_TX_only']\n",
    "\n",
    "data.loc[np.in1d(data['state_id'], 'WI'), 'snap'] = data['snap_WI']\n",
    "data.loc[np.in1d(data['state_id'], 'WI'), 'snap_other'] = data['snap_other_WI']\n",
    "data.loc[np.in1d(data['state_id'], 'WI'), 'snap_only_other'] = data['snap_other_WI_only']\n",
    "\n",
    "data = reduce_mem_usage(data) #Mem. usage decreased to 17213.41 Mb (4.5% reduction)\n",
    "\n",
    "data.drop(['snap_CA', 'snap_TX', 'snap_WI'\n",
    "                , 'snap_other_CA', 'snap_other_TX', 'snap_other_WI'\n",
    "                , 'snap_other_CA_only', 'snap_other_TX_only', 'snap_other_WI_only'], inplace = True, axis = 1)\n",
    "\n",
    "\n",
    "    \n",
    "gc.collect()\n",
    "\n",
    "#Considering that we will estimate a model by product, there are some more variables we can drop.\n",
    "data.drop(['cat_id', 'dept_id', 'item_id', 'state_id', 'store_id'], inplace = True, axis = 1)\n",
    "data = reduce_mem_usage(data) #Mem. usage decreased to 14982.73 Mb (0.0% reduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created variables.\n",
    "\n",
    "Continuous:\\\n",
    "    'month', 'wday',\\\n",
    "    'demand_lag_t1', 'demand_lag_t2', 'demand_lag_t3', 'demand_lag_t4', 'demand_lag_t5',\\\n",
    "    'demand_lag_t6', 'demand_lag_t7', 'demand_lag_t8', 'demand_lag_t9', 'demand_lag_t10', 'demand_lag_t11', 'demand_lag_t12',\\\n",
    "    'demand_lag_t13', 'demand_lag_t14', 'demand_lag_t15', 'demand_lag_t16', 'demand_lag_t17', 'demand_lag_t18',\\\n",
    "    'demand_lag_t19', 'demand_lag_t20', 'demand_lag_t21', 'demand_lag_t22', 'demand_lag_t23', 'demand_lag_t24',\\\n",
    "    'demand_lag_t25', 'demand_lag_t26', 'demand_lag_t27', 'demand_lag_t28',\\\n",
    "    'demand_rolling_mean_t28_7', 'demand_rolling_mean_t28_30', 'demand_rolling_mean_t28_90', 'demand_rolling_mean_t28_180',\n",
    "    'demand_rolling_mean_t28_365', 'demand_rolling_std_t28_7', 'demand_rolling_std_t28_30', 'demand_rolling_std_t28_90',\n",
    "    'demand_rolling_std_t28_180', 'demand_rolling_std_t28_365', 'demand_rolling_skew_t28_30','demand_rolling_kurt_t28_30'\n",
    "    \n",
    "Categorical:\\\n",
    "    'season', 'month_fase', 'week_fase',\\\n",
    "    \"SuperBowl\", 'ValentinesDay', 'PresidentsDay', 'LentStart', 'LentWeek2', 'StPatricksDay', 'Purim End', 'OrthodoxEaster',\\\n",
    "    'Pesach End', 'Cinco De Mayo', \"Mother's day\", 'MemorialDay', 'NBAFinalsStart', 'NBAFinalsEnd', \"Father's day\",\\\n",
    "    'IndependenceDay', 'Ramadan starts', 'Eid al-Fitr', 'LaborDay', 'ColumbusDay', 'Halloween', 'EidAlAdha', 'VeteransDay',\\\n",
    "    'Thanksgiving', 'Christmas', 'Chanukah End', 'NewYear', 'OrthodoxChristmas', 'MartinLutherKingDay',\\\n",
    "    'Easter', 'Sporting', 'Cultural', 'National', 'Religious',\\\n",
    "    'snap', 'snap_other', 'snap_only_other'\n",
    "\n",
    "Holiday variables (categorical):\n",
    " holidays_variables = col for col in data.columns if col.endswith(\"_near\") or col.endswith(\"_week\") or col.endswith(\"_weekend\")\n",
    "Price variables (categorical):\n",
    "price_variables = col for col in variables_aux.columns if 'price' in col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0328\n",
      "Elapsed: 00:12:59\n",
      "Total run time: 06:35:50\n",
      "Time to finish: 06:22:51\n",
      "\n",
      "0.0656\n",
      "Elapsed: 00:25:50\n",
      "Total run time: 06:33:48\n",
      "Time to finish: 06:07:58\n",
      "\n",
      "0.0984\n",
      "Elapsed: 00:38:39\n",
      "Total run time: 06:32:47\n",
      "Time to finish: 05:54:08\n",
      "\n",
      "0.1312\n",
      "Elapsed: 00:51:35\n",
      "Total run time: 06:33:10\n",
      "Time to finish: 05:41:35\n",
      "\n",
      "0.164\n",
      "Elapsed: 01:04:31\n",
      "Total run time: 06:33:24\n",
      "Time to finish: 05:28:53\n",
      "\n",
      "0.1968\n",
      "Elapsed: 01:17:32\n",
      "Total run time: 06:33:58\n",
      "Time to finish: 05:16:26\n",
      "\n",
      "0.2296\n",
      "Elapsed: 01:30:31\n",
      "Total run time: 06:34:14\n",
      "Time to finish: 05:03:43\n",
      "\n",
      "0.2624\n",
      "Elapsed: 01:43:31\n",
      "Total run time: 06:34:30\n",
      "Time to finish: 04:50:59\n",
      "\n",
      "0.2952\n",
      "Elapsed: 01:56:23\n",
      "Total run time: 06:34:15\n",
      "Time to finish: 04:37:52\n",
      "\n",
      "0.328\n",
      "Elapsed: 02:09:16\n",
      "Total run time: 06:34:06\n",
      "Time to finish: 04:24:50\n",
      "\n",
      "0.3608\n",
      "Elapsed: 02:22:16\n",
      "Total run time: 06:34:19\n",
      "Time to finish: 04:12:03\n",
      "\n",
      "0.3936\n",
      "Elapsed: 02:35:09\n",
      "Total run time: 06:34:11\n",
      "Time to finish: 03:59:02\n",
      "\n",
      "0.4264\n",
      "Elapsed: 02:48:07\n",
      "Total run time: 06:34:16\n",
      "Time to finish: 03:46:09\n",
      "\n",
      "0.4592\n",
      "Elapsed: 03:01:06\n",
      "Total run time: 06:34:23\n",
      "Time to finish: 03:33:17\n",
      "\n",
      "0.492\n",
      "Elapsed: 03:14:12\n",
      "Total run time: 06:34:43\n",
      "Time to finish: 03:20:31\n",
      "\n",
      "0.5248\n",
      "Elapsed: 03:27:12\n",
      "Total run time: 06:34:49\n",
      "Time to finish: 03:07:37\n",
      "\n",
      "0.5576\n",
      "Elapsed: 03:40:14\n",
      "Total run time: 06:34:58\n",
      "Time to finish: 02:54:44\n",
      "\n",
      "0.5904\n",
      "Elapsed: 03:53:08\n",
      "Total run time: 06:34:52\n",
      "Time to finish: 02:41:44\n",
      "\n",
      "0.6232\n",
      "Elapsed: 04:06:05\n",
      "Total run time: 06:34:52\n",
      "Time to finish: 02:28:47\n",
      "\n",
      "0.656\n",
      "Elapsed: 04:18:57\n",
      "Total run time: 06:34:44\n",
      "Time to finish: 02:15:47\n",
      "\n",
      "0.6888\n",
      "Elapsed: 04:31:49\n",
      "Total run time: 06:34:37\n",
      "Time to finish: 02:02:48\n",
      "\n",
      "0.7216\n",
      "Elapsed: 04:44:51\n",
      "Total run time: 06:34:45\n",
      "Time to finish: 01:49:54\n",
      "\n",
      "0.7544\n",
      "Elapsed: 04:57:48\n",
      "Total run time: 06:34:45\n",
      "Time to finish: 01:36:57\n",
      "\n",
      "0.7872\n",
      "Elapsed: 05:10:39\n",
      "Total run time: 06:34:38\n",
      "Time to finish: 01:23:59\n",
      "\n",
      "0.82\n",
      "Elapsed: 05:23:29\n",
      "Total run time: 06:34:30\n",
      "Time to finish: 01:11:01\n",
      "\n",
      "0.8528\n",
      "Elapsed: 05:36:22\n",
      "Total run time: 06:34:26\n",
      "Time to finish: 00:58:04\n",
      "\n",
      "0.8856\n",
      "Elapsed: 05:49:16\n",
      "Total run time: 06:34:23\n",
      "Time to finish: 00:45:07\n",
      "\n",
      "0.9184\n",
      "Elapsed: 06:02:03\n",
      "Total run time: 06:34:13\n",
      "Time to finish: 00:32:10\n",
      "\n",
      "0.9512\n",
      "Elapsed: 06:14:56\n",
      "Total run time: 06:34:10\n",
      "Time to finish: 00:19:14\n",
      "\n",
      "0.984\n",
      "Elapsed: 06:27:49\n",
      "Total run time: 06:34:07\n",
      "Time to finish: 00:06:18\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Considering that we will model an ARIMA by product, we will already construct a database separed by product.\n",
    "#The ideal would be to use the key function of hdf files, but this would exceed the recomented nunber of children (16.384).\n",
    "#Therefore, we will make one file by product.\n",
    "\n",
    "products_ids = data['id'].unique()\n",
    "products_ids_size = len(products_ids)\n",
    "\n",
    "progress = 0   #Usefull to see the progress of the code\n",
    "progress_1000 = 1\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "for e in products_ids:\n",
    "    data.loc[np.in1d(data['id'], e)].to_hdf(path + e + '.h5', key = e, format = 't', mode = 'w')\n",
    "    \n",
    "    progress += 1\n",
    "    if progress > progress_1000 * 1000:\n",
    "\n",
    "        progress_per = round(progress / products_ids_size, 4)\n",
    "        print(progress_per)\n",
    "        progress_1000 +=1\n",
    "        \n",
    "        end = time.time()\n",
    "        elapsed = int(round(end - start, 0))\n",
    "        total_run_time =  int(round(elapsed / (progress_per), 0))\n",
    "        time_to_finish = int(round(elapsed / (progress_per), 0)) - elapsed\n",
    "        print('Elapsed: {:02d}:{:02d}:{:02d}'.format(elapsed // 3600, (elapsed % 3600 // 60), elapsed % 60))\n",
    "        print('Total run time: {:02d}:{:02d}:{:02d}'.format(total_run_time // 3600, (total_run_time % 3600 // 60), total_run_time % 60))\n",
    "        print('Time to finish: {:02d}:{:02d}:{:02d}'.format(time_to_finish // 3600, (time_to_finish % 3600 // 60), time_to_finish % 60))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#Observation: HDF5 files does not support variables formated as category.\n",
    "\n",
    "#Define variables as categorics\n",
    "categoric_variables = ['season', 'month_fase', 'week_fase', \"SuperBowl\", \n",
    "       'ValentinesDay', 'PresidentsDay', 'LentStart', 'LentWeek2', 'StPatricksDay', 'Purim End', 'OrthodoxEaster', 'Pesach End',\n",
    "       'Cinco De Mayo', \"Mother's day\", 'MemorialDay', 'NBAFinalsStart', 'NBAFinalsEnd', \"Father's day\", 'IndependenceDay', 'Ramadan starts',\n",
    "       'Eid al-Fitr', 'LaborDay', 'ColumbusDay', 'Halloween', 'EidAlAdha', 'VeteransDay', 'Thanksgiving', 'Christmas', 'Chanukah End', 'NewYear',\n",
    "       'OrthodoxChristmas', 'MartinLutherKingDay', 'Easter', 'Sporting', 'Cultural', 'National', 'Religious',\n",
    "        'snap', 'snap_other', 'snap_only_other']\n",
    "\n",
    "#Holiday variables:\n",
    "holidays_variables = [col for col in data.columns if col.endswith(\"_near\") or col.endswith(\"_week\") or col.endswith(\"_weekend\")]\n",
    "for e in holidays_variables:\n",
    "    categoric_variables.append(e)\n",
    "    \n",
    "#Price variables:\n",
    "price_variables = [col for col in data.columns if 'price' in col][1:]\n",
    "for e in price_variables:\n",
    "    categoric_variables.append(e)\n",
    "    \n",
    "for e in categoric_variables:\n",
    "    data[e] = data[e].astype('category')\n",
    "    \n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
